{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1dvWORIUL3zWi4Df4hNUeHYfzPM_YRRGs\n",
      "To: /home/yuuhanase/FPTU/EXE101/PaperClipAI_EnglishGrading/experiments/train.csv\n",
      "100%|██████████████████████████████████████| 9.29M/9.29M [00:00<00:00, 9.34MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1dvWORIUL3zWi4Df4hNUeHYfzPM_YRRGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-06 20:43:57,588] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "class CustomBERTModel(nn.Module):\n",
    "      def __init__(self):\n",
    "            super(CustomBERTModel, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "            ### New layers:\n",
    "            self.linear1 = nn.Linear(768, 6)\n",
    "            self.output = nn.Sigmoid()\n",
    "\n",
    "      def forward(self, **inputs):\n",
    "            bert_outputs = self.bert(**inputs)\n",
    "\n",
    "            # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "            linear1_output = self.linear1(bert_outputs.pooler_output) ## extract the 1st token's embeddings\n",
    "            output = self.output(linear1_output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "class CustomTrainer(Trainer):\n",
    "    def __int__(self, *args, **kwargs):\n",
    "        super().__int__(*args, **kwargs)\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs['target']\n",
    "        # forward pass\n",
    "        logits = model(\n",
    "            input_ids=inputs['input_ids'].to('cuda'),\n",
    "            attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "        )\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(logits[-1], labels)\n",
    "        return (loss, logits) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBERTModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (output): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = CustomBERTModel() # You can pass the parameters if required to have more flexible model\n",
    "model.to(torch.device(\"cuda\")) ## can be gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "\n",
    "def clean_text(batch):\n",
    "    text = batch['full_text']\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    text = text.replace(\"\\r\", ' ')\n",
    "    \n",
    "    text = clean_extra_whitespace(text)\n",
    "    batch['full_text'] = text\n",
    "    return batch\n",
    "\n",
    "def transform(batch):\n",
    "    tokenized_input = tokenizer(batch['full_text'], return_tensors='pt', truncation=True)\n",
    "    input_ids = tokenized_input['input_ids'][0]\n",
    "    attention_mask = tokenized_input['attention_mask'][0]\n",
    "    targets_feat = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    targets = []\n",
    "    for feat in targets_feat:\n",
    "        targets.append(torch.tensor(batch[feat])/5.0)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target': targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9f3d433ddd4c64b6791c2fdef6ba47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f581e01fbd44d2fa11bd697dc905254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9504d2a8ac4e421493f35885df01725f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
       "    num_rows: 3911\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"train.csv\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b0a0125d76425e8ac703560fc1bdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
       "     num_rows: 3911\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'input_ids', 'attention_mask', 'target'],\n",
       "     num_rows: 3911\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'target'],\n",
       "     num_rows: 3911\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ds = dataset.map(clean_text)\n",
    "transform_ds = clean_ds.map(transform)\n",
    "train_ds = Dataset.from_dict({\n",
    "    'input_ids': transform_ds['input_ids'],\n",
    "    'attention_mask': transform_ds['attention_mask'],\n",
    "    'target': transform_ds['target']\n",
    "})\n",
    "clean_ds, transform_ds, train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1045,\n",
       "  2228,\n",
       "  2008,\n",
       "  2493,\n",
       "  2052,\n",
       "  5770,\n",
       "  2013,\n",
       "  4083,\n",
       "  2012,\n",
       "  2188,\n",
       "  1010,\n",
       "  2138,\n",
       "  2027,\n",
       "  2180,\n",
       "  2102,\n",
       "  2031,\n",
       "  2000,\n",
       "  2689,\n",
       "  1998,\n",
       "  2131,\n",
       "  2039,\n",
       "  2220,\n",
       "  1999,\n",
       "  1996,\n",
       "  2851,\n",
       "  2000,\n",
       "  6457,\n",
       "  1998,\n",
       "  2079,\n",
       "  2045,\n",
       "  2606,\n",
       "  1012,\n",
       "  2635,\n",
       "  2069,\n",
       "  4280,\n",
       "  7126,\n",
       "  2068,\n",
       "  2138,\n",
       "  2012,\n",
       "  2045,\n",
       "  2160,\n",
       "  2027,\n",
       "  1005,\n",
       "  2222,\n",
       "  2022,\n",
       "  3477,\n",
       "  2062,\n",
       "  3086,\n",
       "  1012,\n",
       "  2027,\n",
       "  2097,\n",
       "  2022,\n",
       "  6625,\n",
       "  2012,\n",
       "  2188,\n",
       "  1012,\n",
       "  1996,\n",
       "  18263,\n",
       "  2112,\n",
       "  1997,\n",
       "  2082,\n",
       "  2003,\n",
       "  2893,\n",
       "  3201,\n",
       "  1012,\n",
       "  2017,\n",
       "  5256,\n",
       "  2039,\n",
       "  2175,\n",
       "  8248,\n",
       "  2115,\n",
       "  4091,\n",
       "  1998,\n",
       "  2175,\n",
       "  2000,\n",
       "  2115,\n",
       "  9346,\n",
       "  1998,\n",
       "  2298,\n",
       "  2012,\n",
       "  2115,\n",
       "  8416,\n",
       "  2015,\n",
       "  1012,\n",
       "  2044,\n",
       "  2017,\n",
       "  2228,\n",
       "  2017,\n",
       "  3856,\n",
       "  1037,\n",
       "  11018,\n",
       "  1057,\n",
       "  2175,\n",
       "  2298,\n",
       "  1999,\n",
       "  1996,\n",
       "  5259,\n",
       "  1998,\n",
       "  2017,\n",
       "  3363,\n",
       "  2593,\n",
       "  2025,\n",
       "  2066,\n",
       "  2009,\n",
       "  2030,\n",
       "  2017,\n",
       "  2298,\n",
       "  1998,\n",
       "  2156,\n",
       "  1037,\n",
       "  21101,\n",
       "  1012,\n",
       "  2059,\n",
       "  2017,\n",
       "  1005,\n",
       "  2222,\n",
       "  2031,\n",
       "  2000,\n",
       "  2689,\n",
       "  1012,\n",
       "  2007,\n",
       "  1996,\n",
       "  3784,\n",
       "  4280,\n",
       "  2017,\n",
       "  2064,\n",
       "  4929,\n",
       "  2505,\n",
       "  1998,\n",
       "  2994,\n",
       "  2188,\n",
       "  1998,\n",
       "  2017,\n",
       "  2180,\n",
       "  2102,\n",
       "  2342,\n",
       "  2000,\n",
       "  6911,\n",
       "  2055,\n",
       "  2054,\n",
       "  2000,\n",
       "  4929,\n",
       "  1012,\n",
       "  2087,\n",
       "  2493,\n",
       "  2788,\n",
       "  2202,\n",
       "  23442,\n",
       "  2077,\n",
       "  2082,\n",
       "  1012,\n",
       "  2027,\n",
       "  2593,\n",
       "  2202,\n",
       "  2009,\n",
       "  2077,\n",
       "  2027,\n",
       "  3637,\n",
       "  2030,\n",
       "  2043,\n",
       "  2027,\n",
       "  5256,\n",
       "  2039,\n",
       "  1012,\n",
       "  2070,\n",
       "  2493,\n",
       "  2079,\n",
       "  2119,\n",
       "  2000,\n",
       "  5437,\n",
       "  2204,\n",
       "  1012,\n",
       "  2008,\n",
       "  5320,\n",
       "  2068,\n",
       "  2079,\n",
       "  3335,\n",
       "  1996,\n",
       "  3902,\n",
       "  1998,\n",
       "  3896,\n",
       "  2006,\n",
       "  2045,\n",
       "  10800,\n",
       "  2051,\n",
       "  3426,\n",
       "  2027,\n",
       "  2272,\n",
       "  2397,\n",
       "  2000,\n",
       "  2082,\n",
       "  1012,\n",
       "  2043,\n",
       "  1057,\n",
       "  2031,\n",
       "  3784,\n",
       "  4280,\n",
       "  1057,\n",
       "  2180,\n",
       "  2102,\n",
       "  2342,\n",
       "  2000,\n",
       "  3335,\n",
       "  8220,\n",
       "  3426,\n",
       "  2017,\n",
       "  2064,\n",
       "  2131,\n",
       "  2673,\n",
       "  2275,\n",
       "  2039,\n",
       "  1998,\n",
       "  2175,\n",
       "  2202,\n",
       "  1037,\n",
       "  6457,\n",
       "  1998,\n",
       "  2043,\n",
       "  1057,\n",
       "  2131,\n",
       "  2041,\n",
       "  2115,\n",
       "  3201,\n",
       "  2000,\n",
       "  2175,\n",
       "  1012,\n",
       "  2043,\n",
       "  2115,\n",
       "  2188,\n",
       "  2115,\n",
       "  6625,\n",
       "  1998,\n",
       "  2017,\n",
       "  3477,\n",
       "  3086,\n",
       "  1012,\n",
       "  2009,\n",
       "  3957,\n",
       "  2059,\n",
       "  2019,\n",
       "  5056,\n",
       "  2000,\n",
       "  2022,\n",
       "  25670,\n",
       "  1998,\n",
       "  2130,\n",
       "  3413,\n",
       "  2045,\n",
       "  19846,\n",
       "  2006,\n",
       "  2465,\n",
       "  2147,\n",
       "  1012,\n",
       "  2270,\n",
       "  2816,\n",
       "  2024,\n",
       "  3697,\n",
       "  2130,\n",
       "  2065,\n",
       "  2017,\n",
       "  3046,\n",
       "  1012,\n",
       "  2070,\n",
       "  3836,\n",
       "  2123,\n",
       "  2102,\n",
       "  2113,\n",
       "  2129,\n",
       "  2000,\n",
       "  6570,\n",
       "  2009,\n",
       "  1999,\n",
       "  2059,\n",
       "  2126,\n",
       "  2008,\n",
       "  2493,\n",
       "  3305,\n",
       "  2009,\n",
       "  1012,\n",
       "  2008,\n",
       "  5320,\n",
       "  2493,\n",
       "  2000,\n",
       "  8246,\n",
       "  1998,\n",
       "  2027,\n",
       "  2089,\n",
       "  9377,\n",
       "  1996,\n",
       "  2465,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'target': [0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.6000000238418579,\n",
       "  0.6000000238418579,\n",
       "  0.800000011920929,\n",
       "  0.6000000238418579]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuuhanase/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0188, 'learning_rate': 4.9989775051124746e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0229, 'learning_rate': 4.997955010224949e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0102, 'learning_rate': 4.996932515337424e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0098, 'learning_rate': 4.995910020449898e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0226, 'learning_rate': 4.9948875255623726e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0318, 'learning_rate': 4.993865030674847e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0175, 'learning_rate': 4.992842535787321e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0251, 'learning_rate': 4.9918200408997955e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0118, 'learning_rate': 4.9907975460122705e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0258, 'learning_rate': 4.989775051124745e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0149, 'learning_rate': 4.988752556237219e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0094, 'learning_rate': 4.9877300613496935e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0256, 'learning_rate': 4.986707566462168e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0203, 'learning_rate': 4.985685071574642e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0253, 'learning_rate': 4.984662576687117e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0071, 'learning_rate': 4.983640081799591e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0239, 'learning_rate': 4.982617586912066e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0109, 'learning_rate': 4.98159509202454e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0129, 'learning_rate': 4.9805725971370145e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0168, 'learning_rate': 4.979550102249489e-05, 'epoch': 0.02}\n",
      "{'loss': 0.018, 'learning_rate': 4.978527607361964e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0225, 'learning_rate': 4.9775051124744375e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0149, 'learning_rate': 4.9764826175869125e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0087, 'learning_rate': 4.975460122699387e-05, 'epoch': 0.02}\n",
      "{'loss': 0.011, 'learning_rate': 4.974437627811861e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0205, 'learning_rate': 4.9734151329243355e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0229, 'learning_rate': 4.9723926380368105e-05, 'epoch': 0.03}\n",
      "{'loss': 0.01, 'learning_rate': 4.971370143149284e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0509, 'learning_rate': 4.970347648261759e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0176, 'learning_rate': 4.9693251533742335e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0123, 'learning_rate': 4.968302658486708e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0112, 'learning_rate': 4.967280163599182e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0345, 'learning_rate': 4.966257668711657e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0114, 'learning_rate': 4.965235173824131e-05, 'epoch': 0.03}\n",
      "{'loss': 0.029, 'learning_rate': 4.964212678936606e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     17\u001b[0m                         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     18\u001b[0m                         train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds,\n\u001b[1;32m     19\u001b[0m                         data_collator\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, DataCollatorWithPadding\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"log_level\": \"error\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"report_to\": \"none\",\n",
    "    \"full_determinism\": False\n",
    "}\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    remove_unused_columns=False,\n",
    "    **default_args)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = CustomTrainer(model=model, \n",
    "                        args=training_args,\n",
    "                        train_dataset=train_ds,\n",
    "                        data_collator=data_collator)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
