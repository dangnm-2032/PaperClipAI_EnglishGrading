{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 09:11:16.623306: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 09:11:16.623328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 09:11:16.623863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 09:11:17.025952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-12 09:11:18,060] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel, PreTrainedModel\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class CustomT5Model(PreTrainedModel):\n",
    "    def __init__(self, config, base_model):\n",
    "        super(CustomT5Model, self).__init__(config)\n",
    "        self.t5 = T5EncoderModel.from_pretrained(\n",
    "            base_model,\n",
    "            config=config\n",
    "        )\n",
    "        ### New layers:\n",
    "        self.regression_layer = nn.Sequential(\n",
    "            nn.AvgPool2d((1548, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(config.hidden_size, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, 6),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, **inputs):\n",
    "        t5_outputs = self.t5(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'])\n",
    "        logits = self.regression_layer(t5_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        self.t5._init_weights(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 09:11:20.210008: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:504] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-12.2\n",
      "  /usr/local/cuda\n",
      "  /home/yuuhanase/miniconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/yuuhanase/miniconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-01-12 09:11:20.250521: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2024-01-12 09:11:20.250642: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:207] INTERNAL: Generating device code failed.\n",
      "2024-01-12 09:11:20.250906: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Exception encountered when calling layer 'layer_norm' (type TFT5LayerNorm).\n\n{{function_node __wrapped__Rsqrt_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Rsqrt] name: \n\nCall arguments received by layer 'layer_norm' (type TFT5LayerNorm):\n  â€¢ hidden_states=tf.Tensor(shape=(1, 2, 1024), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomT5Model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;66;03m#.cuda()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3236\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3236\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m, in \u001b[0;36mCustomT5Model.__init__\u001b[0;34m(self, config, base_model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, base_model):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CustomT5Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt5 \u001b[38;5;241m=\u001b[39m \u001b[43mTFT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m### New layers:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregression_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     14\u001b[0m         nn\u001b[38;5;241m.\u001b[39mAvgPool2d((\u001b[38;5;241m1548\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     15\u001b[0m         nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# nn.Sigmoid(),\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2912\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2910\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild()  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2912\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m   2915\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1140\u001b[0m, in \u001b[0;36mTFPreTrainedModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# Set the serving spec quickly to ensure that Keras doesn't use the specific dummy input shapes as the spec\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# Setting it in build() allows users to override the shape when loading a non-pretrained model from config\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_save_spec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:1529\u001b[0m, in \u001b[0;36mTFT5EncoderModel.call\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(T5_ENCODER_INPUTS_DOCSTRING)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;129m@replace_return_docstrings\u001b[39m(output_type\u001b[38;5;241m=\u001b[39mTFBaseModelOutput, config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1510\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, TFBaseModelOutput]:\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;124;03m    >>> outputs = model(input_ids)\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1529\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1545\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:778\u001b[0m, in \u001b[0;36mTFT5MainLayer.call\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    777\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 778\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoder_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001b[39;00m\n\u001b[1;32m    795\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m layer_outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:567\u001b[0m, in \u001b[0;36mTFT5Block.call\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    578\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:456\u001b[0m, in \u001b[0;36mTFT5LayerSelfAttention.call\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    447\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m ):\n\u001b[0;32m--> 456\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    458\u001b[0m         normed_hidden_states,\n\u001b[1;32m    459\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m], training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:92\u001b[0m, in \u001b[0;36mTFT5LayerNorm.call\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     91\u001b[0m     variance \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msquare(hidden_states), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\n",
      "\u001b[0;31mUnknownError\u001b[0m: Exception encountered when calling layer 'layer_norm' (type TFT5LayerNorm).\n\n{{function_node __wrapped__Rsqrt_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Rsqrt] name: \n\nCall arguments received by layer 'layer_norm' (type TFT5LayerNorm):\n  â€¢ hidden_states=tf.Tensor(shape=(1, 2, 1024), dtype=float32)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "base_model = \"google/flan-t5-large\"\n",
    "checkpoint = '/home/yuuhanase/FPTU/EXE101/PaperClipAI_EnglishGrading/artifacts/trained_model/EnglishGrading_t5_regression_5e'\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = CustomT5Model.from_pretrained(\n",
    "    checkpoint,\n",
    "    base_model=base_model,\n",
    "    config=config,\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "\n",
    "def clean_text(batch):\n",
    "    text = batch['full_text']\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    text = text.replace(\"\\r\", ' ')\n",
    "    \n",
    "    text = clean_extra_whitespace(text)\n",
    "    batch['full_text'] = text\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(batch):\n",
    "    tokenized_input = tokenizer(\n",
    "        batch['full_text'], \n",
    "        return_tensors='pt', \n",
    "        padding='max_length',\n",
    "        max_length=1548)\n",
    "    input_ids = tokenized_input['input_ids'][0]\n",
    "    attention_mask = tokenized_input['attention_mask'][0]\n",
    "    targets_feat = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    targets = []\n",
    "    for feat in targets_feat:\n",
    "        targets.append(batch[feat]/5.0)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': torch.Tensor(targets)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 11 21:59:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   51C    P2             101W / 370W |   1681MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1178      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1363      G   /usr/bin/gnome-shell                          8MiB |\n",
      "|    0   N/A  N/A     25016      C   /home/yuuhanase/miniconda3/bin/python      1650MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cohesion': 2.5,\n",
       " 'syntax': 3.0,\n",
       " 'vocabulary': 3.0,\n",
       " 'phraseology': 2.5,\n",
       " 'grammar': 2.5,\n",
       " 'conventions': 2.5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(inp_text):\n",
    "    tokenized_input = tokenizer(\n",
    "        inp_text, \n",
    "        return_tensors='pt',\n",
    "        max_length=1548,\n",
    "        padding='max_length').to(model.device)\n",
    "    output = model(**tokenized_input)[0]\n",
    "    output\n",
    "    feats = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    result = {}\n",
    "    for i in range(6):\n",
    "        result[feats[i]] = round((output[i].item()*5.0)*2)/2.0\n",
    "    tokenized_input = tokenized_input.to('cpu')\n",
    "    output = output.to('cpu')\n",
    "\n",
    "    del tokenized_input, output\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "inp_text = \"When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A problem is a change for you because it can make you see different and help you to understand how tings wok. First of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me because everything was different that my country and then i realist that wrong because a problem may change you but sometimes can not change the way it is, but i remember that i was really shy but i think that change a lot because sometimes my problems make me think that there is more thing that i never see in my life but i just need to see it from a different way and dont let nothing happened and ruing the change that i want to make because of just a problem. For example i think that nothing was going to change me and that i dont need to be shy anymore became i need to start seeing everything in a different ways because you can get mad at every one but you need to know what is going to happened after, people may see you different but the only way that you know how to change is to do the best and don't let nothing or not body to change nothing about you. The way you want to change not one have that and can't do nothing about it because is your choice and your problems and you can decide what to do with it. second of all can help you to understand how things work. For instance my mom have a lot of problems but she have faith when she is around people, my mom is scare of high and i'm not scare of high i did not understand why my mos is scare of high and in not scare of high and every time i see my mom in a airplane it make me laugh because she is scare and is funny, but i see it from a different way and i like the high but also she have to understand that hoe things work in other people because it can no be the same as you. For example i think that my mom and me are different because we are and i have to understand that she does not like high and i need to understand that. to help someone to understand how things work you need to start to see how things work in that persons life. A problem is a change for you and can make you a different and help you to understand. Everyone has a different opinion and a different was to understand then others. everyone can see the different opinion and what other people think.\"\n",
    "#2.5,2.5,3.0,2.0,2.0,2.5\n",
    "inference(inp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load DATASET\n",
      "Dataset({\n",
      "    features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
      "    num_rows: 7822\n",
      "})\n",
      "CLEAN\n",
      "Dataset({\n",
      "    features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
      "    num_rows: 7822\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "print(\"Load DATASET\")\n",
    "dataset = load_dataset('tasksource/english-grading', split='train')\n",
    "print(dataset)\n",
    "print(\"CLEAN\")\n",
    "clean_ds = dataset.map(clean_text)\n",
    "print(clean_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch):\n",
    "    results = inference(batch['full_text'])\n",
    "    sum = 0\n",
    "    for result in results:\n",
    "        loss = abs(batch[result] - results[result])\n",
    "        batch[f'loss_{result}'] = loss\n",
    "        sum += loss\n",
    "    batch['loss_mean'] = sum/6\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b4de8e58324539b298834ea3a8453a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'loss_cohesion', 'loss_syntax', 'loss_vocabulary', 'loss_phraseology', 'loss_grammar', 'loss_conventions', 'loss_mean'],\n",
       "    num_rows: 7822\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = clean_ds.map(evaluate)\n",
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>loss_cohesion</th>\n",
       "      <th>loss_syntax</th>\n",
       "      <th>loss_vocabulary</th>\n",
       "      <th>loss_phraseology</th>\n",
       "      <th>loss_grammar</th>\n",
       "      <th>loss_conventions</th>\n",
       "      <th>loss_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal If u change the school policy ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal If u change the school policy ...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  loss_cohesion  \\\n",
       "0     3.5         3.0          3.0      4.0          3.0            0.0   \n",
       "1     2.5         3.0          2.0      2.0          2.5            0.0   \n",
       "2     3.5         3.0          3.0      3.0          2.5            0.0   \n",
       "3     4.5         4.5          4.5      4.0          5.0            0.5   \n",
       "4     3.0         3.0          3.0      2.5          2.5            0.5   \n",
       "\n",
       "   loss_syntax  loss_vocabulary  loss_phraseology  loss_grammar  \\\n",
       "0          0.0              0.5               0.5           0.5   \n",
       "1          0.5              0.0               0.5           0.5   \n",
       "2          0.5              0.0               0.0           0.0   \n",
       "3          0.5              0.0               0.5           0.5   \n",
       "4          0.0              0.0               0.0           0.5   \n",
       "\n",
       "   loss_conventions  loss_mean  \n",
       "0               0.5   0.333333  \n",
       "1               0.0   0.250000  \n",
       "2               0.5   0.166667  \n",
       "3               0.0   0.333333  \n",
       "4               0.5   0.250000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(val_ds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>loss_cohesion</th>\n",
       "      <th>loss_syntax</th>\n",
       "      <th>loss_vocabulary</th>\n",
       "      <th>loss_phraseology</th>\n",
       "      <th>loss_grammar</th>\n",
       "      <th>loss_conventions</th>\n",
       "      <th>loss_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "      <td>7822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.127077</td>\n",
       "      <td>3.028254</td>\n",
       "      <td>3.235745</td>\n",
       "      <td>3.116850</td>\n",
       "      <td>3.032856</td>\n",
       "      <td>3.081053</td>\n",
       "      <td>0.389926</td>\n",
       "      <td>0.333291</td>\n",
       "      <td>0.293148</td>\n",
       "      <td>0.357325</td>\n",
       "      <td>0.361161</td>\n",
       "      <td>0.380977</td>\n",
       "      <td>0.352638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.644358</td>\n",
       "      <td>0.583111</td>\n",
       "      <td>0.655955</td>\n",
       "      <td>0.699796</td>\n",
       "      <td>0.671407</td>\n",
       "      <td>0.346448</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.295366</td>\n",
       "      <td>0.336155</td>\n",
       "      <td>0.329712</td>\n",
       "      <td>0.338224</td>\n",
       "      <td>0.140564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cohesion       syntax   vocabulary  phraseology      grammar  \\\n",
       "count  7822.000000  7822.000000  7822.000000  7822.000000  7822.000000   \n",
       "mean      3.127077     3.028254     3.235745     3.116850     3.032856   \n",
       "std       0.662500     0.644358     0.583111     0.655955     0.699796   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       2.500000     2.500000     3.000000     2.500000     2.500000   \n",
       "50%       3.000000     3.000000     3.000000     3.000000     3.000000   \n",
       "75%       3.500000     3.500000     3.500000     3.500000     3.500000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "       conventions  loss_cohesion  loss_syntax  loss_vocabulary  \\\n",
       "count  7822.000000    7822.000000  7822.000000      7822.000000   \n",
       "mean      3.081053       0.389926     0.333291         0.293148   \n",
       "std       0.671407       0.346448     0.316129         0.295366   \n",
       "min       1.000000       0.000000     0.000000         0.000000   \n",
       "25%       2.500000       0.000000     0.000000         0.000000   \n",
       "50%       3.000000       0.500000     0.500000         0.500000   \n",
       "75%       3.500000       0.500000     0.500000         0.500000   \n",
       "max       5.000000       3.000000     1.500000         1.500000   \n",
       "\n",
       "       loss_phraseology  loss_grammar  loss_conventions    loss_mean  \n",
       "count       7822.000000   7822.000000       7822.000000  7822.000000  \n",
       "mean           0.357325      0.361161          0.380977     0.352638  \n",
       "std            0.336155      0.329712          0.338224     0.140564  \n",
       "min            0.000000      0.000000          0.000000     0.000000  \n",
       "25%            0.000000      0.000000          0.000000     0.250000  \n",
       "50%            0.500000      0.500000          0.500000     0.333333  \n",
       "75%            0.500000      0.500000          0.500000     0.416667  \n",
       "max            2.500000      2.500000          2.000000     2.083333  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'loss_cohesion'}>,\n",
       "        <Axes: title={'center': 'loss_syntax'}>,\n",
       "        <Axes: title={'center': 'loss_vocabulary'}>],\n",
       "       [<Axes: title={'center': 'loss_phraseology'}>,\n",
       "        <Axes: title={'center': 'loss_grammar'}>,\n",
       "        <Axes: title={'center': 'loss_conventions'}>],\n",
       "       [<Axes: title={'center': 'loss_mean'}>, <Axes: >, <Axes: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(['loss_cohesion', 'loss_syntax', 'loss_vocabulary', 'loss_phraseology', 'loss_grammar', 'loss_conventions', 'loss_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 09:35:11.480364: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 09:35:11.480386: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 09:35:11.480910: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 09:35:11.874362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaebc84989774bddb319f40e5fd71bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18570995cc394680af7e3fefed053e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d3cf05fc074bc5890bd69457b8ec4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a07262f1a01435d9ff1e31b5b9257d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e8195677d64736a53f2ee2df5d27b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 09:35:40.213606: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:504] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-12.2\n",
      "  /usr/local/cuda\n",
      "  /home/yuuhanase/miniconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/yuuhanase/miniconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-01-12 09:35:40.254537: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2024-01-12 09:35:40.254662: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:207] INTERNAL: Generating device code failed.\n",
      "2024-01-12 09:35:40.254922: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Exception encountered when calling layer 'layer_norm' (type TFT5LayerNorm).\n\n{{function_node __wrapped__Rsqrt_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Rsqrt] name: \n\nCall arguments received by layer 'layer_norm' (type TFT5LayerNorm):\n  â€¢ hidden_states=tf.Tensor(shape=(1, 2, 512), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TFT5EncoderModel\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudies have been shown that owning a dog is good for you\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\u001b[38;5;241m.\u001b[39minput_ids  \u001b[38;5;66;03m# Batch size 1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2912\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2910\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild()  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2912\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m   2915\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1140\u001b[0m, in \u001b[0;36mTFPreTrainedModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# Set the serving spec quickly to ensure that Keras doesn't use the specific dummy input shapes as the spec\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# Setting it in build() allows users to override the shape when loading a non-pretrained model from config\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_save_spec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:1529\u001b[0m, in \u001b[0;36mTFT5EncoderModel.call\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(T5_ENCODER_INPUTS_DOCSTRING)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;129m@replace_return_docstrings\u001b[39m(output_type\u001b[38;5;241m=\u001b[39mTFBaseModelOutput, config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1510\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, TFBaseModelOutput]:\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;124;03m    >>> outputs = model(input_ids)\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1529\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1545\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:778\u001b[0m, in \u001b[0;36mTFT5MainLayer.call\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    777\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 778\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoder_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001b[39;00m\n\u001b[1;32m    795\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m layer_outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:567\u001b[0m, in \u001b[0;36mTFT5Block.call\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    578\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:456\u001b[0m, in \u001b[0;36mTFT5LayerSelfAttention.call\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    447\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m ):\n\u001b[0;32m--> 456\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    458\u001b[0m         normed_hidden_states,\n\u001b[1;32m    459\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m], training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_tf_t5.py:92\u001b[0m, in \u001b[0;36mTFT5LayerNorm.call\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     91\u001b[0m     variance \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msquare(hidden_states), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\n",
      "\u001b[0;31mUnknownError\u001b[0m: Exception encountered when calling layer 'layer_norm' (type TFT5LayerNorm).\n\n{{function_node __wrapped__Rsqrt_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Rsqrt] name: \n\nCall arguments received by layer 'layer_norm' (type TFT5LayerNorm):\n  â€¢ hidden_states=tf.Tensor(shape=(1, 2, 512), dtype=float32)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFT5EncoderModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = TFT5EncoderModel.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"tf\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model(input_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
