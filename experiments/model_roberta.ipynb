{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, PreTrainedModel\n",
    "from torch import nn\n",
    "class CustomROBERTAModel(PreTrainedModel):\n",
    "      def __init__(self, config, transformer_model_name_or_path, num_feats):\n",
    "            super(CustomROBERTAModel, self).__init__(config)\n",
    "            self.roberta = RobertaModel.from_pretrained(\n",
    "                  transformer_model_name_or_path,\n",
    "                  config=config\n",
    "            )\n",
    "            ### New layers:\n",
    "            self.num_feats = num_feats\n",
    "            self.output_layers = []\n",
    "            for _ in range(self.num_feats):\n",
    "                  self.output_layers.append(\n",
    "                        nn.Sequential(\n",
    "                              nn.Linear(config.hidden_size, 512),\n",
    "                              nn.GELU(),\n",
    "                              nn.Linear(512, 256),\n",
    "                              nn.GELU(),\n",
    "                              nn.Linear(256, 128),\n",
    "                              nn.GELU(),\n",
    "                              nn.Linear(128, 64),\n",
    "                              nn.GELU(),\n",
    "                              nn.Linear(64, 32),\n",
    "                              nn.GELU(),\n",
    "                              nn.Linear(32, 9),\n",
    "                              nn.Softmax(),\n",
    "                        ).to('cuda')\n",
    "                  )\n",
    "\n",
    "      def forward(self, **inputs):\n",
    "            roberta_outputs = self.roberta(**inputs)\n",
    "            outputs = []\n",
    "            for i in range(self.num_feats):\n",
    "                  outputs.append(\n",
    "                        self.output_layers[i](roberta_outputs.pooler_output)[0]\n",
    "                  )\n",
    "            logits = outputs\n",
    "            return logits\n",
    "\n",
    "      def _init_weights(self, module):\n",
    "            self.bert._init_weights(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import torch\n",
    "base_model = 'roberta-large'\n",
    "checkpoint = '/home/yuuhanase/FPTU/EXE101/PaperClipAI_EnglishGrading/EnglishGradingModel'\n",
    "config = AutoConfig.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = CustomROBERTAModel(\n",
    "    config=config, \n",
    "    transformer_model_name_or_path=base_model, \n",
    "    num_feats=6\n",
    ").to(\"cuda\") ## can be gpu\n",
    "# model = CustomROBERTAModel.from_pretrained(\n",
    "#     checkpoint, \n",
    "#     config=config, \n",
    "#     transformer_model_name_or_path=base_model,\n",
    "#     num_feats=6\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuuhanase/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1226, 0.1213, 0.1031, 0.1220, 0.0926, 0.1157, 0.0938, 0.1252, 0.1037]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1172, 0.0983, 0.1352, 0.1339, 0.0981, 0.1073, 0.0976, 0.0954, 0.1171]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1154, 0.1133, 0.1052, 0.1126, 0.1049, 0.1033, 0.0999, 0.1227, 0.1226]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1038, 0.1091, 0.1067, 0.1053, 0.1242, 0.1250, 0.1276, 0.1055, 0.0927]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1023, 0.1231, 0.1073, 0.1098, 0.1221, 0.1122, 0.0965, 0.1054, 0.1213]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1016, 0.1342, 0.1001, 0.1168, 0.1270, 0.1106, 0.1010, 0.1011, 0.1076]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt').to(\"cuda\")\n",
    "output = model(**encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0384, -0.0209, -0.0141,  ..., -0.0523,  0.0802,  0.0632],\n",
       "         [ 0.3952,  0.0993, -0.6829,  ...,  0.1277, -0.4668,  0.4225],\n",
       "         [-0.0097, -0.0010,  0.0426,  ..., -0.0275,  0.0668,  0.0367]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8011, -0.4364,  0.2403,  ..., -0.4839,  0.1486, -0.3779]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, AutoConfig\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "config = AutoConfig.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "text = \"Hello\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0384, -0.0209, -0.0141,  ..., -0.0523,  0.0802,  0.0632],\n",
       "         [ 0.3952,  0.0993, -0.6829,  ...,  0.1277, -0.4668,  0.4225],\n",
       "         [-0.0097, -0.0010,  0.0426,  ..., -0.0275,  0.0668,  0.0367]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8011, -0.4364,  0.2403,  ..., -0.4839,  0.1486, -0.3779]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cohesion': 2.383091002702713,\n",
       " 'syntax': 2.0666779577732086,\n",
       " 'vocabulary': 2.884359359741211,\n",
       " 'phraseology': 2.844240665435791,\n",
       " 'grammar': 2.9789209365844727,\n",
       " 'conventions': 2.2683973610401154}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_text = \"When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A problem is a change for you because it can make you see different and help you to understand how tings wok. First of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me because everything was different that my country and then i realist that wrong because a problem may change you but sometimes can not change the way it is, but i remember that i was really shy but i think that change a lot because sometimes my problems make me think that there is more thing that i never see in my life but i just need to see it from a different way and dont let nothing happened and ruing the change that i want to make because of just a problem. For example i think that nothing was going to change me and that i dont need to be shy anymore became i need to start seeing everything in a different ways because you can get mad at every one but you need to know what is going to happened after, people may see you different but the only way that you know how to change is to do the best and don't let nothing or not body to change nothing about you. The way you want to change not one have that and can't do nothing about it because is your choice and your problems and you can decide what to do with it. second of all can help you to understand how things work. For instance my mom have a lot of problems but she have faith when she is around people, my mom is scare of high and i'm not scare of high i did not understand why my mos is scare of high and in not scare of high and every time i see my mom in a airplane it make me laugh because she is scare and is funny, but i see it from a different way and i like the high but also she have to understand that hoe things work in other people because it can no be the same as you. For example i think that my mom and me are different because we are and i have to understand that she does not like high and i need to understand that. to help someone to understand how things work you need to start to see how things work in that persons life. A problem is a change for you and can make you a different and help you to understand. Everyone has a different opinion and a different was to understand then others. everyone can see the different opinion and what other people think.\"\n",
    "#2.5,2.5,3.0,2.0,2.0,2.5\n",
    "tokenized_input = tokenizer(inp_text, return_tensors='pt', truncation=True)#.to(torch.device(\"cuda\"))\n",
    "output = model(**tokenized_input)[0]\n",
    "feats = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "result = {}\n",
    "for i in range(6):\n",
    "    result[feats[i]] = output[i].item()*5.0\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "class CustomTrainer(Trainer):\n",
    "    def __int__(self, *args, **kwargs):\n",
    "        super().__int__(*args, **kwargs)\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        logits = model(**inputs)\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, logits) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
